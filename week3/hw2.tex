\documentclass{tufte-book}

\usepackage{amsmath, amsthm}
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

\title{STAT  245\\Homework 2}
\author{Joe Seidel}
\date{\today}

\input{../preamble.tex}

\begin{document}

\maketitle
\pagenumbering{gobble}
\newpage
\pagenumbering{arabic}

\subsection{1. Cauchy Distribution}
Let $X$ and $Y$ be two independent $N(0,1)$ random variables.  Show the distribution $X/Y$ is the same as that of $X/|Y|=X/\sqrt{Y^2}$.  This means that $X/Y$ has a $t_1$-distribution which is also known as Cauchy-distribution.

First find $\frac{X}{Y} \sim$ Cauchy.  We have $X,Y \sim N(0,1)$ and $X$ and $Y$ are independent.  Let $W=\frac{X}{Y}$ then
\begin{align*}
f_W(w) &= \int_{-\infty}^{\infty}f_{X,Y}(wy,y)|y|dy \\
&= \int_{-\infty}^{\infty} \frac{1}{2\pi}e^{-\frac{w^2y^2 +y^2}{2}}ydy\\
&= \frac{1}{\pi(1+w^2)}.
\end{align*}

Now let $Z=\frac{X}{-Y}$ then
\begin{align*}
f_Z(z) &= \int_{-\infty}^{\infty}f_{X,Y}(-zy,-y)|-y|dy \\
&= \int_{-\infty}^{\infty} \frac{1}{2\pi}e^{-\frac{z^2y^2 +y^2}{2}}ydy\\
&= \frac{1}{\pi(1+z^2)}.
\end{align*}

Hence
\[ \frac{X}{Y}=\frac{X}{-Y}\Rightarrow \frac{X}{|Y|}=\frac{X}{\sqrt{Y^2}}=\frac{X}{\pm Y}. \]

Alternatively, by symmetry of $N(0,1)$ over $0$, $X=-X$.  Which allows for the same conclusion.


\subsection{2. Bivariate normal distribution I} Suppose $(X,Y)$ has a bivariate normal distribution with expected values $\mathbb{E}(X)=3$ and $\mathbb{E}(Y)=1$, variances $\Var(X)=9$ and $\Var(Y)=16$, with correlation $\rho$.  Let $W_a=12+aX+Y$ and $V=19+X+2Y$.

\begin{enumerate}

\item[(a)] Fix $\rho=1/3$ and find $a \in \mathbb{R}$ such that $W_a$ and $V$ are independent.  Can you choose $p_0 \in (-1,1)$ such that there does exist an $a\in \mathbb{R}$ making $W_a$ and $V$ independent?  If yes, final all such $p_0$.  If no, explain why not.

Since these are normally distributed variables we can use the Gaussain assumption, which says that normally distributed variables are independent if covariance is $0$.

\begin{align*}
\Cov(W_a, V) &= \Cov(aX+Y, X+2Y)\\
&= a\Cov(X,X) + \Cov(X,Y) + 2a\Cov(X,Y) + 2\Cov(Y,Y)\\
&= a\Var(X) + \Cov(X,Y)(1+2a) + 2\Var(Y)\\
&= 9a + 12\rho(1+2a) + 32.
\end{align*}

Fixing $\rho=1/3$
\[ \Cov(W_a,V) = 17a + 36 \]
therefore when $a= -\frac{36}{17}$, $\Cov(W_a,V)=0\Rightarrow$ $W,V$ independent.

Furthermore, solve
\[  9a + 12\rho(1+2a) + 32 = 0\]
for $a$, then $a=-\frac{4(3\rho + 8)}{24\rho + 9}$ where $8\rho +3 \neq0$.  Let $\rho=-\frac{3}{8}$ then there doesn't exists an $a\in \mathbb{R}$ such that $X,Y$ are independent.

\item[(b)] Now fix $a=1$ and find $\rho$ such that $W_a$ and $V$ are independent.  Can you choose $a_0 \in \mathbb{R}$ such that there does not exist a $\rho \in (-1,1)$ making $W_{a_o}$ and $V$ independent?  If yes, find all such $a_0$.  If no, explain why not.

Fixing $a=1$
\[ \Cov(W,V) = 9+12\rho(3) + 32 \]
and solving for $0$, $\rho=-\frac{41}{36}$.  Since $\rho \not\in (-1,1)$ choose $a_0=a=1$ then there does not exists $\rho \in (-1,1)$ such that $W_a,V$ are independent.

\end{enumerate}

\subsection{3. Bivariate normal distribution II}
Let $(X,Y)$ follow a bivariate normal distribution with $\mathbb{E}(X)=5$ and $\mathbb{E}(Y)=3$, variances $\Var(X)=9$ and $\Var(Y)=16$, and correlation $\rho=0.4$.  Find

\begin{enumerate}

\item[(a)] the conditional expectation $\mathbb{E}(X \ | \ Y=8)$,
In general, the conditional expectation for normal distributed variables is
\[ E(X \mid Y) = u_Y + \rho \frac{\sigma_Y}{\sigma_X}(x-\mu_x). \]

Then
\[ E[X \mid Y=8 ] = 5 + .4\frac{3}{4}(8-3) = 6.5 \].

\item[(b)] the condtional variance $\Var(X \mid Y=8)$.
In general, the conditional variance for normal distributed random variables is
\[ \Var[Y \mid X] = \sigma_Y^2(1-\rho^2). \]

Then
\[ \Var[X \mid Y=8] = 9(1-.4^2) = 7.56. \]

\item[(c)] The probability $\Pr(3 < X < 5)$.

Since $X \sim N(5,9)$ we re-arrange the inequality
\begin{align*}
3 <& X < 5\\
\frac{3-5}{3} <& \frac{X-5}{3} < \frac{5-5}{3}.
\end{align*}

Now evaluate
\begin{align*}
\Pr(-\frac{2}{3} < Z < 0 ) &= \Pr(Z < 0) - \Pr(Z<-\frac{2}{3}) \\
&= .247
\end{align*}
where $Z \sim N(0,1)$.

\item[(d)]The conditional probablity $\Pr(3<X<5 \mid Y=8)$.

Using facts from parts (a) and (b), $X\mid Y \sim N(6.5, 7.56)$.  Then
\begin{align*}
\Pr(3<X<5 \mid Y=8) &= \Pr(X<5 \mid Y=8) - \Pr(X<3 \mid Y=8)\\
&= \Pr\Big( \frac{x-6.5}{\sqrt{7.56}} < \frac{-1.5}{\sqrt{7.56}}\Big) - \Pr\Big(\frac{x-6.5}{\sqrt{7.56}} < \frac{-3.5}{\sqrt{7.56}}\Big) \\
&= \Pr\Big( Z < \frac{-1.5}{\sqrt{7.56}}\Big) - \Pr\Big(Z < \frac{-3.5}{\sqrt{7.56}}\Big)\\
&= .1911
\end{align*}
where $Z \sim N(0,1)$.
\end{enumerate}

\subsection{4. Joint distributions}
Let $X$ and $Y$ be the scores of a Stat 245 student on midterm and final exam.  We model these scores as
\[ X=S+E_1, \ Y=S+E_2, \]
where $S, E_1, E_2$ are independent random variables distributed as $S \sim N(70,49)$, $E_1,E_2 \sim N(0,25)$.  We think of $S$ as a "skill" part of the score and $E_1, E_2$ as "luck" components.

\begin{enumerate}
\item[(a)] What is the joint distribution of $(X,Y)$?

The general form is \marginnote{See Rice p.81}
\[ f_{X,Y} = \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}} \exp\big(-\frac{1}{2(1-\rho^2)}\big[ \frac{(y-\mu_Y)^2}{\sigma_x^2}+\frac{(x-\mu_X)^2}{\sigma_Y^2} - \frac{2\rho(x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_Y} \big]\big) \]
where
\[ \rho=\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}. \]

Compute
\begin{align*}
\Cov(X,Y) &= \Cov(S+E_1,S+E_2)\\
&=\Cov(S,S) + \Cov(E_1,S) + \Cov(S,E_2) + \Cov(E_1,E_2)\\
&= \Var(S) + 0 + 0 + 0\\
&= 49
\end{align*}
in order to find
\[ \rho=\frac{49}{74}. \]

Plug in the values to find the joint distrubution.

\item[(b)]Assume that a student recieved a midterm score that is one standard deviation below the midterm mean.  What do you expect his/her final score to be?

Since $X=\mu_x - \sigma_x$
\begin{align*}
E[Y\mid X] &= \mu_y + \rho \frac{\sigma_Y}{\sigma_X}(\mu_x - \sigma_x - \mu_x)\\
&= \mu_y + \rho \frac{\sigma_Y}{\sigma_X}(- \sigma_x)\\
&= \mu_y - \rho \cdot \sigma_Y\\
&= 70 - \frac{49}{74} \cdot \sqrt{49+25}\\
&= 64.3
\end{align*}

\end{enumerate}

\subsection{5. Mean square error when estimating a normal variance}
Let $X_1,...,X_n$ be independent $N(\mu,\sigma^2)$ random variables.  Let $\overline{X}=\frac{1}{n}\sum_{i=1}^nX_i$ be the sample mean.  Consider two estimators of $sigma^2$, name the sample varaince
\[ s^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2 \]
and the MLE
\[ \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(X_i-\overline{X})^2.\]

The mean square error (MSE) measures how far on average these estimators are away from the "target" $\sigma^2$, where "away" is measured in square distance.  The two MSE are defined as
\[ \text{MSE}(s^2) = E[(s^2-\sigma^2)^2] \text{ and MSE}(\hat{\sigma}^2)= E[(\hat{\sigma}^2 - \sigma^2)^2].\]

\begin{enumerate}
\item[(a)] Compute and compare MSE$(s^2)$ and MSE$(\hat{\sigma}^2)$.

First observe
\begin{align*}
E(\hat{\theta} - \theta) &= E(\hat{\theta}^2) + E(\theta^2) - 2\theta E(\hat{\theta})\\
&= \Var(\hat{\theta}) + [E(\hat{\theta}]^2 + \theta^2 - 2\theta E(\hat{\theta})\\
&= \Var(\hat{\theta}) + [E(\hat{\theta}) - \theta]^2.
\end{align*}

The second term is commonly reffered to as Bias$^2$, then
\[ \text{Bais}(\hat{\theta})=E(\hat{\theta})-\theta. \]

To find MSE$(s^2)$ first find the variance by re-arranging $s^2$ such that
\[ \frac{(n-1)s^2}{\sigma^2} = \frac{\sum_{i=1}^n(x_i -\overline{x})^2}{\sigma^2} \sim \chi_{n-1}^2. \]

Then
\[ E\Big[\frac{(n-1)s^2}{\sigma^2} = n-1 \Rightarrow E(s^2)=\sigma^2 \]
and
\[ \Var\Big[\frac{(n-1)s^2}{\sigma^2}\Big] = 2(n-1) \Rightarrow \Var(s^2)=\frac{2\sigma^4}{n-1}. \]

Now MSE can be calculated
\begin{align*}
\text{MSE}(s^2) &= \Var(s^2) + \text{Bias}(s^2)^2\\
&=\frac{2\sigma^4}{n-1} + (E(s^2) - \sigma^2)^2\\
&= \frac{2\sigma^4}{n-1}
\end{align*}
which means $s^2$ is an unbiased estimator.

For $\hat{\sigma}^2$ observe
\[ \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(X_i-\overline{X})^2 = \frac{n-1}{n}s^2 \]
which can be used to find
\begin{align*}
\Var(\hat{\sigma}^2)&= \frac{(n-1)^2}{n^2}\Var(s^2)\\
&=\frac{(n-1)^2}{n^2}\frac{2\sigma^4}{n-1}\\
&= \frac{2(n-1)\sigma^4}{n^2}.
\end{align*}

Next
\begin{align*}
\text{Bias}(\hat{\sigma}^2=E(\hat{\sigma}^2) - \sigma^2\\
&=E\Big(\frac{n-1}{n}s^2\Big) - \sigma^2\\
&= \frac{n-1}{n}\sigma^2 - \sigma^2.
\end{align*}


The MSE is then

\[ \text{MSE}(\hat{\sigma}^2) =\frac{2n-1}{n^2}\sigma^4 + \Big(\frac{n-1}{n}\sigma^2 - \sigma^2\Big)^2 = \frac{2n-1}{n^2}\sigma^4. \]

Comparing the two
\[ \text{MSE}(\hat{\sigma}^2) < \frac{2n}{n^2}\sigma^4 < \frac{2\sigma^4}{n-1} = \text{MSE}(s^2). \]

\item[(b)]Consider a general form of the estimator
\[ \tilde{\sigma}^2 = c\sum_{i=1}^n(X_i-\overline{X})^2 .\]
Find the best $c$ such that MSE$(\tilde{\sigma}^2)=E[(\tilde{\sigma}^2-\sigma^2)^2]$ is minimized.

First observe
\[  \tilde{\sigma}^2 = c\sum_{i=1}^n(X_i-\overline{X})^2 = c(n-1)s^2 \]

and let $t=c(n-1)$.  Then
\[ E(\tilde{\sigma}^2)= tE(s^2) = t\sigma^2 \]
and
\[\Var(\tilde{\sigma}^2) = t^2\Var(s^2)= \frac{2t^2}{n-1}\sigma^4. \]

Using the above facts
\begin{align*}
\text{MSE}(\tilde{\sigma}^2)&= \Var(\tilde{\sigma}^2) +(t\sigma^2-\sigma^2)^2\\
&= \Var(\tilde{\sigma}^2) + (t-1)^2\sigma^4\\
&= t^2\Var(s^2)= \frac{2t^2}{n-1}\sigma^4 +(t-1)^2\sigma^4\\
&= f(t)\sigma^4
\end{align*}

where
\[ f(t)=\frac{2t^2}{n-1} +(t-1)^2 = \frac{n+1}{n-1}t^2 - 2t+1.\]

By differentiating, $f(t)=\frac{2}{n+1}$, its minimal value, when $t=\frac{n-1}{n+1}$.  Hence the smallest value of MSE$(\tilde{\sigma}^2)=\frac{2\sigma^4}{n+1}$ with
\[(n-1)c=t=\frac{n-1}{n+1} \]
which means $c=\frac{1}{n+1}$
\end{enumerate}
\end{document}
\grid
